# Steps

1. chunk source doc
2. embed source doc chunks
3. create prompt to send question + context from embed
4. LLM returns content